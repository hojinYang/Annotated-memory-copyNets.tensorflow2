{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 bAbi Dataset이 어떻게 구성되어 있는지 살펴보고, 이를 어떻게 학습에 맞게 바꿀 수 있을지 고민하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 Mary moved to the bathroom.\\n', '2 John went to the hallway.\\n', '3 Where is Mary? \\tbathroom\\t1\\n', '4 Daniel went back to the hallway.\\n', '5 Sandra moved to the garden.\\n', '6 Where is Daniel? \\thallway\\t4\\n', '7 John moved to the office.\\n', '8 Sandra journeyed to the bathroom.\\n', '9 Where is Daniel? \\thallway\\t4\\n', '10 Mary moved to the hallway.\\n', '11 Daniel travelled to the office.\\n', '12 Where is Daniel? \\toffice\\t11\\n', '13 John went back to the garden.\\n', '14 John moved to the bedroom.\\n', '15 Where is Sandra? \\tbathroom\\t8\\n', '1 Sandra travelled to the office.\\n', '2 Sandra went to the bathroom.\\n', '3 Where is Sandra? \\tbathroom\\t2\\n', '4 Mary went to the bedroom.\\n', '5 Daniel moved to the hallway.\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('./data/qa1_single-supporting-fact_train.txt') as f:\n",
    "    lines = f.readlines()\n",
    "print(lines[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Story를 구성하는 문장 중간중간에 Question과 Answer이 tap으로 구분된 문장(QA)이 섞여있다. QA 문장은 Question(1), Answer(2)과 함께 정답의 근거가 되는 Supporting(3)이 함께 제안된다. 모델의 input으로 Story, Question, Answer이 들어가도록 데이터를 preprocessing해야한다. 위의 문장 번호로 예를 들면\n",
    "\n",
    "S:[1,2],Q:[3(1)],A:[3(2)]\n",
    "\n",
    "S:[1,2,4,5],Q:[6(1)],A:[6(2)]\n",
    "\n",
    "S:[1,2,4,5,7,8],Q:[9(1)],A:[9(2)]  \n",
    "\n",
    "\n",
    "실제 구현은 Story를 최대로 저장할 수 있는 크기를 설정한 뒤(memory length), 남은 부분은 zero-padding한다. 만약 memory length를 넘는 story가 존재한다면 story의 앞부분을 잘라낸다.\n",
    "\n",
    "memory_length=5\n",
    "\n",
    "S:[1,2,0,0,0],Q:[3(1)],A:[3(2)]\n",
    "\n",
    "S:[1,2,4,5,0],Q:[6(1)],A:[6(2)]\n",
    "\n",
    "S:[2,4,5,7,8],Q:[9(1)],A:[9(2)]  \n",
    "\n",
    "\n",
    "각각의 sentence는 단어로 구성되어 있으며, 문장 역시 최대 길이를 설정한 뒤 남은 부분은 zero-padding한다. 결과적으로 Story의 dimension은 \n",
    "\n",
    "\n",
    "batch_size X memory_length X sentence_length X word_embedding_dimension 이 될 것이다.\n",
    "\n",
    "\n",
    "그럼 우선 문장을 tokeinze하는 함수를 구현한 뒤, dataset를 만들어주는 함수를 구현하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary', 'moved', 'to', 'the', 'bathroom', '.']\n",
      "['Daniel', 'travelled', 'to', 'the', 'office', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hojin\\Anaconda3\\envs\\tensorflow2\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [w.strip() for w in re.split(\"(\\W+)?\", sentence) if w.strip()]\n",
    "\n",
    "print(tokenize(\"Mary moved to the bathroom.\\n\"))\n",
    "print(tokenize(\"Daniel travelled to the office.\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_SQA(lines):\n",
    "    \"\"\"return data: [[story,question,answer,supporting id]]\"\"\"\n",
    "    data = []\n",
    "    story_len = []\n",
    "    sentence_len = []\n",
    "    story = None\n",
    "    num_questions = None\n",
    "    for line in lines:\n",
    "        line.lower()\n",
    "        nid, line = line.split(' ',1)\n",
    "        nid = int(nid)\n",
    "    \n",
    "        if nid == 1:\n",
    "            story = [] # init story\n",
    "            num_questions = [0] #init num_questions\n",
    "            question_count = 0\n",
    "            \n",
    "        if '\\t' not in line: #normal story sentence if '\\t' is not in line\n",
    "            line = tokenize(line)\n",
    "            line = line[:-1] if line[-1] == '.' else line\n",
    "            story.append(line)\n",
    "            sentence_len.append(len(line))\n",
    "    \n",
    "        else : #QA sentence if '\\t' is in the line\n",
    "            q, a, sid = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            q = q[:-1] if q[-1] == '?' else q\n",
    "            sid = int(sid) - num_questions[int(sid)]\n",
    "            data.append([story[:], q, a, sid])\n",
    "            story_len.append(len(story))\n",
    "            question_count += 1\n",
    "            \n",
    "        num_questions.append(question_count) #need to match sentence index without question index\n",
    "            \n",
    "    return data, story_len, sentence_len       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Mary', 'moved', 'to', 'the', 'bathroom'], ['John', 'went', 'to', 'the', 'hallway'], ['Daniel', 'went', 'back', 'to', 'the', 'hallway'], ['Sandra', 'moved', 'to', 'the', 'garden']], ['Where', 'is', 'Daniel'], 'hallway', 3]\n",
      "The longest story length:10\n",
      "The longest sentence length:6\n"
     ]
    }
   ],
   "source": [
    "data,story_len, sentence_len = split_SQA(lines)\n",
    "print(data[1])\n",
    "print(\"The longest story length:{0}\\nThe longest sentence length:{1}\".format(max(story_len), max(sentence_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 긴 story의 길이는 10이며, 가장 긴 sentence의 길이는 6이다. 제법 단순한 문장들로 구성되어 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary(lines):\n",
    "    word2idx = {}\n",
    "    idx = 1\n",
    "    for line in lines:\n",
    "        line.lower()\n",
    "        _, line = line.split(' ',1)\n",
    "        if '\\t' in line:\n",
    "            line = line.split('\\t')[0]\n",
    "        line = tokenize(line)\n",
    "        line = line[:-1] if line[-1] is '?' or '.' else line\n",
    "        for w in line:\n",
    "            if w not in word2idx.keys():\n",
    "                word2idx[w] = idx\n",
    "                idx += 1\n",
    "    return word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mary': 1, 'moved': 2, 'to': 3, 'the': 4, 'bathroom': 5, 'John': 6, 'went': 7, 'hallway': 8, 'Where': 9, 'is': 10, 'Daniel': 11, 'back': 12, 'Sandra': 13, 'garden': 14, 'office': 15, 'journeyed': 16, 'travelled': 17, 'bedroom': 18, 'kitchen': 19}\n"
     ]
    }
   ],
   "source": [
    "dic = make_dictionary(lines)\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "등장하는 단어의 수도 무척 제한적이다. 이제 단어들로 이루어진 문장들을 index로 바꾸어 저장하자. 추가적으로 sentence zero padding과 story zero padding을 함께 진행해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def make_batch(data, sentence_len, memory_len, dic):\n",
    "    S = [];Q = []; A = []; Support = []\n",
    "    for story, question, answer, support in data:\n",
    "        #delete front part of story that exceeds memrory length \n",
    "        start = max(len(story) - memory_len,0)\n",
    "        story = story[start:]\n",
    "        #(1)convert words to idx and (2)zero-pad to match the sentence length\n",
    "        story_idx = []\n",
    "        for sentence in story:\n",
    "            story_idx.append([dic[w] for w in sentence] + [0]*(sentence_len - len(sentence)))\n",
    "        #zero-pad to match the memroy length\n",
    "        for _ in range(memory_len - len(story_idx)):\n",
    "            story_idx.append([0]*sentence_len)\n",
    "        \n",
    "        question_idx = [[dic[w] for w in question] + [0]*(sentence_len - len(question))]\n",
    "        answer_idx = dic[answer]\n",
    "        \n",
    "        S.append(story_idx); Q.append(question_idx); A.append(answer_idx); Support.append(support)\n",
    "    return np.array(S),np.array(Q),np.array(A),np.array(Support)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch size가 2인 상태를 예시로 들어 Memory network 내부구조 구현을 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  2  3  4  5  0]\n",
      "  [ 6  7  3  4  8  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]]\n",
      "\n",
      " [[ 1  2  3  4  5  0]\n",
      "  [ 6  7  3  4  8  0]\n",
      "  [11  7 12  3  4  8]\n",
      "  [13  2  3  4 14  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]]]\n"
     ]
    }
   ],
   "source": [
    "ss_len = max(sentence_len)\n",
    "mem_len = max(story_len)\n",
    "S,Q,A,Support = make_batch(data[:2], max(sentence_len), max(story_len), dic)\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10, 6) (2, 1, 6) (2,)\n"
     ]
    }
   ],
   "source": [
    "print(S.shape,Q.shape,A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.layers as layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_a = layers.Embedding(input_dim = len(dic)+1, output_dim=12)\n",
    "emb_b = layers.Embedding(input_dim = len(dic)+1, output_dim=12)\n",
    "emb_c = layers.Embedding(input_dim = len(dic)+1, output_dim=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10, 6, 12)\n",
      "(2, 1, 6, 12)\n",
      "(2, 10, 6, 12)\n"
     ]
    }
   ],
   "source": [
    "a = emb_a(S)\n",
    "print(a.shape)\n",
    "b = emb_b(Q)\n",
    "print(b.shape)\n",
    "c = emb_c(S)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dimension of Story: batch_size X memory_length X sentence_length X word_embedding_dimension\n",
    "\n",
    "dimension of Question: batch_size X 1 X sentence_length X word_embedding_dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_word_emb(sentence_word_idx, sentence_word_emb):\n",
    "    '''\n",
    "    intput: \n",
    "        sentence_word_idx : [batch_size,memory_length,sentence_length]\n",
    "        sentence_word_emb : [batch_size,memory_length,sentence_length,word_emb]\n",
    "    output: \n",
    "        sentence_emb: [batch_size,memory_length,word_emb]\n",
    "        average sentences\n",
    "    '''\n",
    "    # sentence_word_idx ->  not_zero:[batch_size,memory_length,1]\n",
    "    # 1 if word index is not zero, else 0\n",
    "    not_zero = tf.not_equal(sentence_word_idx, 0)\n",
    "    not_zero = tf.cast(tf.expand_dims(not_zero,-1), tf.float32)\n",
    "    \n",
    "    mul = tf.multiply(sentence_word_emb,not_zero)\n",
    "    return tf.reduce_sum(mul,-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10, 12) (2, 1, 12) (2, 10, 12)\n"
     ]
    }
   ],
   "source": [
    "keys = get_avg_word_emb(S,a)\n",
    "query = get_avg_word_emb(Q,b)\n",
    "values = get_avg_word_emb(S,c)\n",
    "\n",
    "print(keys.shape, query.shape, values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_score(keys, query):\n",
    "    '''\n",
    "    input:\n",
    "        keys: [batch, mem_size, sentence_emb]\n",
    "        query: [batch, sentence_emb]\n",
    "    output:\n",
    "        attn_score: [batch,mem_size], \n",
    "        attention socres for each memory\n",
    "    '''\n",
    "    #calcuate dot product\n",
    "    #dot product-> logits: [batch,mem_size]\n",
    "    elemwise_mul = tf.multiply(keys, query)\n",
    "    logits = tf.reduce_sum(elemwise_mul,-1)\n",
    "    \n",
    "    #zero's of logit: padding sentence. set that value as negative inf\n",
    "    logits = logits + tf.cast(tf.equal(logits,0.),tf.float32)*-1e+10\n",
    "    attn_score = tf.nn.softmax(logits)\n",
    "    \n",
    "    return attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.49940035 0.5005996  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.25186282 0.24918939 0.24856508 0.25038275 0.         0.\n",
      "  0.         0.         0.         0.        ]], shape=(2, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "attn_score = get_attention_score(keys,query)\n",
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_memory_represntation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = tf.cast(tf.expand_dims(z,-1), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[[-0.04579781 -0.03127763  0.02413743 ...  0.0434393   0.04954794\n",
      "     0.03501357]\n",
      "   [-0.04754801 -0.02171919 -0.00236537 ... -0.01449496  0.02593795\n",
      "     0.01264102]\n",
      "   [ 0.01127317  0.02023664 -0.0270755  ...  0.03585057 -0.02598071\n",
      "     0.01751279]\n",
      "   [-0.01103199 -0.04447675  0.04603953 ...  0.04143052  0.02226057\n",
      "    -0.0499106 ]\n",
      "   [-0.0175122   0.03206041  0.02658495 ... -0.03840851  0.04460161\n",
      "     0.04957979]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[-0.00634755 -0.03342913  0.00969497 ... -0.03706624 -0.02374871\n",
      "    -0.00558214]\n",
      "   [ 0.04983275 -0.04245949 -0.0041325  ... -0.02977382  0.04058473\n",
      "     0.02706334]\n",
      "   [ 0.01127317  0.02023664 -0.0270755  ...  0.03585057 -0.02598071\n",
      "     0.01751279]\n",
      "   [-0.01103199 -0.04447675  0.04603953 ...  0.04143052  0.02226057\n",
      "    -0.0499106 ]\n",
      "   [-0.00936512 -0.04077234 -0.02926142 ... -0.03386031 -0.03828931\n",
      "     0.04946932]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]]]\n",
      "\n",
      "\n",
      " [[[-0.04579781 -0.03127763  0.02413743 ...  0.0434393   0.04954794\n",
      "     0.03501357]\n",
      "   [-0.04754801 -0.02171919 -0.00236537 ... -0.01449496  0.02593795\n",
      "     0.01264102]\n",
      "   [ 0.01127317  0.02023664 -0.0270755  ...  0.03585057 -0.02598071\n",
      "     0.01751279]\n",
      "   [-0.01103199 -0.04447675  0.04603953 ...  0.04143052  0.02226057\n",
      "    -0.0499106 ]\n",
      "   [-0.0175122   0.03206041  0.02658495 ... -0.03840851  0.04460161\n",
      "     0.04957979]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[-0.00634755 -0.03342913  0.00969497 ... -0.03706624 -0.02374871\n",
      "    -0.00558214]\n",
      "   [ 0.04983275 -0.04245949 -0.0041325  ... -0.02977382  0.04058473\n",
      "     0.02706334]\n",
      "   [ 0.01127317  0.02023664 -0.0270755  ...  0.03585057 -0.02598071\n",
      "     0.01751279]\n",
      "   [-0.01103199 -0.04447675  0.04603953 ...  0.04143052  0.02226057\n",
      "    -0.0499106 ]\n",
      "   [-0.00936512 -0.04077234 -0.02926142 ... -0.03386031 -0.03828931\n",
      "     0.04946932]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[-0.00131656 -0.04437592  0.03360296 ...  0.00221889  0.00748809\n",
      "    -0.01276021]\n",
      "   [ 0.04983275 -0.04245949 -0.0041325  ... -0.02977382  0.04058473\n",
      "     0.02706334]\n",
      "   [-0.03767562  0.0386132  -0.00971944 ...  0.03500124 -0.04782819\n",
      "    -0.00793757]\n",
      "   [ 0.01127317  0.02023664 -0.0270755  ...  0.03585057 -0.02598071\n",
      "     0.01751279]\n",
      "   [-0.01103199 -0.04447675  0.04603953 ...  0.04143052  0.02226057\n",
      "    -0.0499106 ]\n",
      "   [-0.00936512 -0.04077234 -0.02926142 ... -0.03386031 -0.03828931\n",
      "     0.04946932]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]]\n",
      "\n",
      "  [[-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]\n",
      "   [-0.          0.         -0.         ...  0.          0.\n",
      "     0.        ]]]], shape=(2, 10, 6, 12), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 10, 6, 12])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = tf.multiply(o,c)\n",
    "print(t)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=251, shape=(2, 10, 12), dtype=float32, numpy=\n",
       "array([[[-0.11061684, -0.04517652,  0.06732103,  0.10635129,\n",
       "         -0.09996976,  0.08914635, -0.04336112,  0.04540521,\n",
       "          0.0312294 ,  0.06781693,  0.11636735,  0.06483655],\n",
       "        [ 0.03436127, -0.14090106, -0.00473492,  0.0174717 ,\n",
       "          0.02808759, -0.01624556,  0.02488458, -0.02590782,\n",
       "         -0.07966189, -0.02341929, -0.02517342,  0.03855269],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "       [[-0.11061684, -0.04517652,  0.06732103,  0.10635129,\n",
       "         -0.09996976,  0.08914635, -0.04336112,  0.04540521,\n",
       "          0.0312294 ,  0.06781693,  0.11636735,  0.06483655],\n",
       "        [ 0.03436127, -0.14090106, -0.00473492,  0.0174717 ,\n",
       "          0.02808759, -0.01624556,  0.02488458, -0.02590782,\n",
       "         -0.07966189, -0.02341929, -0.02517342,  0.03855269],\n",
       "        [ 0.00171664, -0.11323466,  0.00945362,  0.02173072,\n",
       "         -0.03781469,  0.09917031,  0.03810121, -0.11361292,\n",
       "         -0.00356859,  0.05086709, -0.04176481,  0.02343705],\n",
       "        [-0.0072455 , -0.05422482, -0.02428712,  0.01476408,\n",
       "         -0.04217005,  0.03566773, -0.06736223,  0.00052931,\n",
       "         -0.08679678,  0.12668979,  0.02669456, -0.00648009],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(t,-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2] *",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
