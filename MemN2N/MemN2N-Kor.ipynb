{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 bAbi Dataset이 어떻게 구성되어 있는지 살펴보고, 이를 어떻게 학습에 맞게 바꿀 수 있을지 고민하자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 Mary moved to the bathroom.\\n', '2 John went to the hallway.\\n', '3 Where is Mary? \\tbathroom\\t1\\n', '4 Daniel went back to the hallway.\\n', '5 Sandra moved to the garden.\\n', '6 Where is Daniel? \\thallway\\t4\\n', '7 John moved to the office.\\n', '8 Sandra journeyed to the bathroom.\\n', '9 Where is Daniel? \\thallway\\t4\\n', '10 Mary moved to the hallway.\\n', '11 Daniel travelled to the office.\\n', '12 Where is Daniel? \\toffice\\t11\\n', '13 John went back to the garden.\\n', '14 John moved to the bedroom.\\n', '15 Where is Sandra? \\tbathroom\\t8\\n', '1 Sandra travelled to the office.\\n', '2 Sandra went to the bathroom.\\n', '3 Where is Sandra? \\tbathroom\\t2\\n', '4 Mary went to the bedroom.\\n', '5 Daniel moved to the hallway.\\n']\n"
     ]
    }
   ],
   "source": [
    "with open('./dataset/qa1_single-supporting-fact_train.txt') as f:\n",
    "    lines = f.readlines()\n",
    "print(lines[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Story를 구성하는 문장 중간중간에 Question과 Answer이 tap으로 구분된 문장(QA)이 섞여있다. QA 문장은 Question(1), Answer(2)과 함께 정답의 근거가 되는 Supporting(3)이 함께 제안된다. 모델의 input으로 Story, Question, Answer이 들어가도록 데이터를 preprocessing해야한다. 위의 문장 번호로 예를 들면\n",
    "\n",
    "S:[1,2],Q:[3(1)],A:[3(2)]\n",
    "\n",
    "S:[1,2,4,5],Q:[6(1)],A:[6(2)]\n",
    "\n",
    "S:[1,2,4,5,7,8],Q:[9(1)],A:[9(2)]  \n",
    "\n",
    "\n",
    "실제 구현은 Story를 최대로 저장할 수 있는 크기를 설정한 뒤(memory length), 남은 부분은 zero-padding한다. 만약 memory length를 넘는 story가 존재한다면 story의 앞부분을 잘라낸다.\n",
    "\n",
    "memory_length=5\n",
    "\n",
    "S:[1,2,0,0,0],Q:[3(1)],A:[3(2)]\n",
    "\n",
    "S:[1,2,4,5,0],Q:[6(1)],A:[6(2)]\n",
    "\n",
    "S:[2,4,5,7,8],Q:[9(1)],A:[9(2)]  \n",
    "\n",
    "\n",
    "각각의 sentence는 단어로 구성되어 있으며, 문장 역시 최대 길이를 설정한 뒤 남은 부분은 zero-padding한다. 결과적으로 Story의 dimension은 \n",
    "\n",
    "\n",
    "batch_size X memory_length X sentence_length X word_embedding_dimension 이 될 것이다.\n",
    "\n",
    "\n",
    "그럼 우선 문장을 tokeinze하는 함수를 구현한 뒤, dataset를 만들어주는 함수를 구현하자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mary', 'moved', 'to', 'the', 'bathroom', '.']\n",
      "['Daniel', 'travelled', 'to', 'the', 'office', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hojin\\Anaconda3\\envs\\tensorflow2\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(sentence):\n",
    "    return [w.strip() for w in re.split(\"(\\W+)?\", sentence) if w.strip()]\n",
    "\n",
    "print(tokenize(\"Mary moved to the bathroom.\\n\"))\n",
    "print(tokenize(\"Daniel travelled to the office.\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_SQA(lines):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        lines: list. [num_lines]\n",
    "    return:\n",
    "        data: list. [num_story*[story,question,answer,supporting id]]\n",
    "        story_len: list. lenght of each story(number of sentences in a story)\n",
    "        sentence_len: list. lenght of each sentence(number of words in a sentence)\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    story_len = []\n",
    "    sentence_len = []\n",
    "    story = None\n",
    "    num_questions = None\n",
    "    for line in lines:\n",
    "        line.lower()\n",
    "        nid, line = line.split(' ',1)\n",
    "        nid = int(nid)\n",
    "    \n",
    "        if nid == 1:\n",
    "            story = [] # init story\n",
    "            num_questions = [0] #init num_questions\n",
    "            question_count = 0\n",
    "            \n",
    "        if '\\t' not in line: #normal story sentence if '\\t' is not in line\n",
    "            line = tokenize(line)\n",
    "            line = line[:-1] if line[-1] == '.' else line\n",
    "            story.append(line)\n",
    "            sentence_len.append(len(line))\n",
    "    \n",
    "        else : #QA sentence if '\\t' is in the line\n",
    "            q, a, sid = line.split('\\t')\n",
    "            q = tokenize(q)\n",
    "            q = q[:-1] if q[-1] == '?' else q\n",
    "            sid = int(sid) - num_questions[int(sid)]\n",
    "            data.append([story[:], q, a, sid -1])\n",
    "            story_len.append(len(story))\n",
    "            question_count += 1\n",
    "            \n",
    "        num_questions.append(question_count) #need to match sentence index without question index\n",
    "            \n",
    "    return data, story_len, sentence_len       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['Mary', 'moved', 'to', 'the', 'bathroom'], ['John', 'went', 'to', 'the', 'hallway']], ['Where', 'is', 'Mary'], 'bathroom', 0]\n",
      "\n",
      "\n",
      "[[['Mary', 'moved', 'to', 'the', 'bathroom'], ['John', 'went', 'to', 'the', 'hallway'], ['Daniel', 'went', 'back', 'to', 'the', 'hallway'], ['Sandra', 'moved', 'to', 'the', 'garden']], ['Where', 'is', 'Daniel'], 'hallway', 2]\n",
      "\n",
      "The longest story length:10\n",
      "The longest sentence length:6\n"
     ]
    }
   ],
   "source": [
    "train_data,story_len, sentence_len = split_SQA(lines)\n",
    "print(train_data[0])\n",
    "print('\\n')\n",
    "print(train_data[1])\n",
    "print(\"\\nThe longest story length:{0}\\nThe longest sentence length:{1}\".format(max(story_len), max(sentence_len)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 긴 story의 길이는 10이며, 가장 긴 sentence의 길이는 6이다. 제법 단순한 문장들로 구성되어 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dictionary(lines):\n",
    "    word2idx = {}\n",
    "    idx = 1\n",
    "    for line in lines:\n",
    "        line.lower()\n",
    "        _, line = line.split(' ',1)\n",
    "        if '\\t' in line:\n",
    "            line = line.split('\\t')[0]\n",
    "        line = tokenize(line)\n",
    "        line = line[:-1] if line[-1] is '?' or '.' else line\n",
    "        for w in line:\n",
    "            if w not in word2idx.keys():\n",
    "                word2idx[w] = idx\n",
    "                idx += 1\n",
    "    return word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Mary': 1, 'moved': 2, 'to': 3, 'the': 4, 'bathroom': 5, 'John': 6, 'went': 7, 'hallway': 8, 'Where': 9, 'is': 10, 'Daniel': 11, 'back': 12, 'Sandra': 13, 'garden': 14, 'office': 15, 'journeyed': 16, 'travelled': 17, 'bedroom': 18, 'kitchen': 19}\n"
     ]
    }
   ],
   "source": [
    "dic = make_dictionary(lines)\n",
    "print(dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "등장하는 단어의 수도 무척 제한적이다. 이제 단어들로 이루어진 문장들을 index로 바꾸어 저장하자. 추가적으로 sentence zero padding과 story zero padding을 함께 진행해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def data_preprocess(data, sentence_len, memory_len, dic):\n",
    "    \"\"\"\n",
    "    input: \n",
    "        data: list. [num_story*[story,question,answer,supporting id]]\n",
    "        sentence_len: int. maximum sentence_len. \n",
    "        memory_len: int. maximum story len\n",
    "        dic: dictionary. \n",
    "    return:\n",
    "        S(tory): np.array. [num_story, memory_len, sentence_len].  \n",
    "        Q(uestion): np.array. [num_story, 1, sentence_len]. \n",
    "        A(nswer): np.array. [num_story, ]\n",
    "        Support: np.array. [num_story, ]\n",
    "    \"\"\"\n",
    "    \n",
    "    S = [];Q = []; A = []; Support = []\n",
    "    for story, question, answer, support in data:\n",
    "        #delete front part of story that exceeds memrory length \n",
    "        start = max(len(story) - memory_len,0)\n",
    "        story = story[start:]\n",
    "        #(1)convert words to idx and (2)zero-pad to match the sentence length\n",
    "        story_idx = []\n",
    "        for sentence in story:\n",
    "            story_idx.append([dic[w] for w in sentence] + [0]*(sentence_len - len(sentence)))\n",
    "        #zero-pad to match the memroy length\n",
    "        for _ in range(memory_len - len(story_idx)):\n",
    "            story_idx.append([0]*sentence_len)\n",
    "        \n",
    "        question_idx = [[dic[w] for w in question] + [0]*(sentence_len - len(question))]\n",
    "        \n",
    "        answer_idx = [0] * (len(dic) + 1)\n",
    "        answer_idx[dic[answer]] = 1\n",
    "        \n",
    "        S.append(story_idx); Q.append(question_idx); A.append(answer_idx); Support.append(support)\n",
    "    return np.array(S),np.array(Q),np.array(A),np.array(Support)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch size가 2인 상태를 예시로 들어 Memory network 내부구조 구현을 살펴보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  2  3  4  5  0]\n",
      "  [ 6  7  3  4  8  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]]\n",
      "\n",
      " [[ 1  2  3  4  5  0]\n",
      "  [ 6  7  3  4  8  0]\n",
      "  [11  7 12  3  4  8]\n",
      "  [13  2  3  4 14  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]\n",
      "  [ 0  0  0  0  0  0]]]\n"
     ]
    }
   ],
   "source": [
    "ss_len = max(sentence_len)\n",
    "mem_len = max(story_len)\n",
    "S,Q,A,Support = data_preprocess(train_data[:2], ss_len, mem_len, dic)\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10, 6) (2, 1, 6) (2, 20) (2,)\n"
     ]
    }
   ],
   "source": [
    "print(S.shape,Q.shape,A.shape, Support.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb_dim = 12\n",
    "\n",
    "emb_a = layers.Embedding(input_dim = len(dic)+1, output_dim=word_emb_dim)\n",
    "emb_b = layers.Embedding(input_dim = len(dic)+1, output_dim=word_emb_dim)\n",
    "emb_c = layers.Embedding(input_dim = len(dic)+1, output_dim=word_emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10, 6, 12)\n",
      "(2, 1, 6, 12)\n",
      "(2, 10, 6, 12)\n"
     ]
    }
   ],
   "source": [
    "a = emb_a(S)\n",
    "print(a.shape)\n",
    "b = emb_b(Q)\n",
    "print(b.shape)\n",
    "c = emb_c(S)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dimension of Story: batch_size X memory_length X sentence_length X word_embedding_dimension\n",
    "\n",
    "dimension of Question: batch_size X 1 X sentence_length X word_embedding_dimension\n",
    "\n",
    "정리하면, batch의 크기는 2이고, 각각의 batch마다 10개의 스토리가 있으며, 각각의 스토리는 6개의 문장으로 이루어져 있고, 각각의 문장은 12차원의 word embedding으로 표현되는 단어들의 집합이다. (query는 각각의 batch마다 하나의 질문이 있다).\n",
    "\n",
    "이제 해결해야 할 것은 word embedding들의 집합을 sentence embedding으로 나타내는 것인데, 우선 가장 간단한 방법인 word embedding의 평균으로 문장을 나타내보자. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_word_emb(sentence_word_idx, sentence_word_emb):\n",
    "    '''\n",
    "    intput: \n",
    "        sentence_word_idx : [batch_size,memory_length,sentence_length]\n",
    "        sentence_word_emb : [batch_size,memory_length,sentence_length,word_emb_len]\n",
    "    output: \n",
    "        sentence_emb: [batch_size,memory_length,word_emb_len]\n",
    "        average sentences\n",
    "    '''\n",
    "    # sentence_word_idx ->  not_zero:[batch_size,memory_length,1]\n",
    "    # 1 if word index is not zero, else 0\n",
    "    not_zero = tf.not_equal(sentence_word_idx, 0)\n",
    "    not_zero = tf.cast(tf.expand_dims(not_zero,-1), tf.float32)\n",
    "    \n",
    "    mul = tf.multiply(sentence_word_emb,not_zero)\n",
    "    return tf.reduce_sum(mul,-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 10, 12) (2, 1, 12) (2, 10, 12)\n"
     ]
    }
   ],
   "source": [
    "keys = get_avg_word_emb(S,a)\n",
    "query = get_avg_word_emb(Q,b)\n",
    "values = get_avg_word_emb(S,c)\n",
    "\n",
    "print(keys.shape, query.shape, values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_score(keys, query):\n",
    "    '''\n",
    "    input:\n",
    "        keys: [batch, mem_len, word_emb_len]\n",
    "        query: [batch, 1, word_emb_len]\n",
    "    output:\n",
    "        attn_score: [batch,mem_len], \n",
    "        attention socres for each memory component\n",
    "    '''\n",
    "    #calcuate dot product\n",
    "    #dot product-> logits: [batch,mem_size]\n",
    "    elemwise_mul = tf.multiply(keys, query)\n",
    "    logits = tf.reduce_sum(elemwise_mul,-1)\n",
    "    \n",
    "    #zero's of logit: padding sentence. set that value as negative inf\n",
    "    logits_pad = logits + tf.cast(tf.equal(logits,0.),tf.float32)*-1e+10\n",
    "    attn_score = tf.nn.softmax(logits_pad)\n",
    "    \n",
    "    return attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.5003158  0.49968415 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.24996877 0.24852683 0.24905097 0.25245348 0.         0.\n",
      "  0.         0.         0.         0.        ]], shape=(2, 10), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "attn_score = get_attention_score(keys,query)\n",
    "print(attn_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_memory_represntation(values, attn_score):\n",
    "    '''\n",
    "    input:\n",
    "        values: [batch, mem_len, word_emb_len]\n",
    "        attn_score: [batch,mem_len], \n",
    "    output:\n",
    "        mem_rep: [batch, word_emb_len]\n",
    "    '''\n",
    "    \n",
    "    #attn_score_expand[batch,mem_size,1]\n",
    "    attn_score_expand = tf.expand_dims(attn_score, -1)\n",
    "    #get memory representation\n",
    "    #mul[batch,mem_size,sentence_emb]\n",
    "    mul = tf.multiply(values, attn_score_expand)\n",
    "    mem_rep = tf.reduce_sum(mul, -2)\n",
    "    \n",
    "    return mem_rep\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.01542873 -0.00454633  0.08610645  0.10061193 -0.01662656  0.06419418\n",
      "   0.0125137   0.03442524 -0.03258358 -0.01858172  0.08948081 -0.02875923]\n",
      " [ 0.00693787  0.03351345  0.05264403  0.08770313 -0.00967796  0.05295835\n",
      "   0.00034816  0.01645008 -0.02345917 -0.00802074  0.0505798  -0.01337768]], shape=(2, 12), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mem_rep = get_output_memory_represntation(values, attn_score)\n",
    "print(mem_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemLayer(layers.Layer):\n",
    "    def __init__(self, vocab_size, word_emb_dim):\n",
    "        super(MemLayer, self).__init__()\n",
    "        self.emb_a = layers.Embedding(vocab_size+1, word_emb_dim,input_length=sentence_len)\n",
    "        self.emb_b = layers.Embedding(vocab_size+1, word_emb_dim,input_length=sentence_len)\n",
    "        self.emb_c = layers.Embedding(vocab_size+1, word_emb_dim,input_length=sentence_len)\n",
    "        \n",
    "        self.logit_layer = layers.Dense(vocab_size + 1)\n",
    "        \n",
    "    def call(self, story, question):\n",
    "        a = self.emb_a(story); b = self.emb_b(question); c = self.emb_c(story)\n",
    "\n",
    "        keys = get_avg_word_emb(story, a)     #[batch_size,memory_length,word_emb_dim]\n",
    "        query = get_avg_word_emb(question, b) #[batch_size,1,word_emb_dim]\n",
    "        values = get_avg_word_emb(story, c)   #[batch_size,memory_length,word_emb_dim]\n",
    "        \n",
    "        attn_score = get_attention_score(keys,query) #[batch_size,memory_length]\n",
    "        mem_rep = get_output_memory_represntation(values, attn_score) #[batch_size,word_emb_dim]\n",
    "        query_squeeze = tf.squeeze(query) #[batch_size,word_emb_dim]\n",
    "        out = mem_rep + query_squeeze #[batch_size,word_emb_dim]\n",
    "        \n",
    "        return out, attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.06885671  0.01596903  0.04857868  0.0793507   0.00252632 -0.05488843\n",
      "  -0.04818626 -0.0033795  -0.04938336  0.09648502  0.13865146 -0.00148977]\n",
      " [-0.02715708  0.11025342  0.04472348  0.10957113 -0.04149476 -0.0640047\n",
      "  -0.06871129  0.0434243  -0.03897819  0.09421811  0.05388987  0.01761121]], shape=(2, 12), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = MemLayer(len(dic), 12)\n",
    "out, attn_score = model(S,Q)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleMemN2N(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, word_emb_dim):\n",
    "        super(SingleMemN2N, self).__init__()\n",
    "        self.mem_layer = MemLayer(vocab_size, word_emb_dim)\n",
    "        self.logit_layer = layers.Dense(vocab_size + 1)\n",
    "    \n",
    "    def call(self, story, question):\n",
    "        out, attn_score = self.mem_layer(story, question)\n",
    "        logit = self.logit_layer(out)\n",
    "        \n",
    "        return logit,attn_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(data, sentence_len, memory_len, dic, batch_size):\n",
    "    S,Q,A,Support = data_preprocess(data, sentence_len, memory_len, dic)\n",
    "    loader = tf.data.Dataset.from_tensor_slices((S,Q,A,Support))\n",
    "    loader = loader.shuffle(buffer_size=len(S)).batch(batch_size)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hojin\\Anaconda3\\envs\\tensorflow2\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "with open('./dataset/qa1_single-supporting-fact_test.txt') as f:\n",
    "    t_lines = f.readlines()\n",
    "test_data,_,_ = split_SQA(t_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_emb_dim = 128\n",
    "lr = 0.0001\n",
    "batch_size = 250\n",
    "epochs = 500\n",
    "model = SingleMemN2N(len(dic), word_emb_dim)\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch20 loss:10.8058\n",
      "correct ratio: 0.1780 correct attn ratio: 0.2220\n",
      "epoch40 loss:9.3333\n",
      "correct ratio: 0.1940 correct attn ratio: 0.2510\n",
      "epoch60 loss:8.1698\n",
      "correct ratio: 0.2080 correct attn ratio: 0.2270\n",
      "epoch80 loss:7.5357\n",
      "correct ratio: 0.2650 correct attn ratio: 0.2270\n",
      "epoch100 loss:7.2016\n",
      "correct ratio: 0.3210 correct attn ratio: 0.2370\n",
      "epoch120 loss:6.9909\n",
      "correct ratio: 0.3480 correct attn ratio: 0.2520\n",
      "epoch140 loss:6.8275\n",
      "correct ratio: 0.3590 correct attn ratio: 0.2580\n",
      "epoch160 loss:6.6787\n",
      "correct ratio: 0.3820 correct attn ratio: 0.2850\n",
      "epoch180 loss:6.5262\n",
      "correct ratio: 0.4030 correct attn ratio: 0.3170\n",
      "epoch200 loss:6.3564\n",
      "correct ratio: 0.4320 correct attn ratio: 0.3550\n",
      "epoch220 loss:6.1571\n",
      "correct ratio: 0.4540 correct attn ratio: 0.4350\n",
      "epoch240 loss:5.9178\n",
      "correct ratio: 0.5040 correct attn ratio: 0.5180\n",
      "epoch260 loss:5.6310\n",
      "correct ratio: 0.5490 correct attn ratio: 0.5690\n",
      "epoch280 loss:5.2959\n",
      "correct ratio: 0.5970 correct attn ratio: 0.5930\n",
      "epoch300 loss:4.9245\n",
      "correct ratio: 0.6340 correct attn ratio: 0.5870\n",
      "epoch320 loss:4.5429\n",
      "correct ratio: 0.6580 correct attn ratio: 0.5920\n",
      "epoch340 loss:4.1820\n",
      "correct ratio: 0.6750 correct attn ratio: 0.6110\n",
      "epoch360 loss:3.8640\n",
      "correct ratio: 0.6670 correct attn ratio: 0.6070\n",
      "epoch380 loss:3.5975\n",
      "correct ratio: 0.6660 correct attn ratio: 0.6100\n",
      "epoch400 loss:3.3801\n",
      "correct ratio: 0.6690 correct attn ratio: 0.5990\n",
      "epoch420 loss:3.2045\n",
      "correct ratio: 0.6710 correct attn ratio: 0.5970\n",
      "epoch440 loss:3.0622\n",
      "correct ratio: 0.6730 correct attn ratio: 0.5990\n",
      "epoch460 loss:2.9457\n",
      "correct ratio: 0.6740 correct attn ratio: 0.6060\n",
      "epoch480 loss:2.8490\n",
      "correct ratio: 0.6730 correct attn ratio: 0.6040\n",
      "epoch500 loss:2.7676\n",
      "correct ratio: 0.6720 correct attn ratio: 0.6030\n"
     ]
    }
   ],
   "source": [
    "train_loader = get_data_loader(train_data, ss_len, mem_len, dic, batch_size)\n",
    "test_loader = get_data_loader(test_data, ss_len, mem_len, dic, batch_size)\n",
    "\n",
    "for epoch in range(1,epochs+1):\n",
    "    total_loss = 0\n",
    "    for batch_id, batch in enumerate(train_loader):\n",
    "        story,question,answer,support = batch[0], batch[1], batch[2], batch[3] \n",
    "        # Open a GradientTape.\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Forward pass.\n",
    "            logit, attn_score = model(story, question)\n",
    "            # Loss value for this batch.\n",
    "            loss = loss_fn(answer,logit)\n",
    "            # Get gradients of weights wrt the loss.\n",
    "            gradients  = tape.gradient(loss, model.trainable_weights)\n",
    "            # Update the weights of our linear layer.\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
    "            total_loss += float(loss)\n",
    "    if epoch%20 == 0:\n",
    "        print(\"epoch{0} loss:{1:.4f}\".format(epoch, total_loss))\n",
    "        correct_ratio = []\n",
    "        correct_attn_ratio = []\n",
    "        for batch in test_loader:\n",
    "            story,question,answer,support = batch[0], batch[1], batch[2], batch[3] \n",
    "            logit, attn_score = model(story, question)\n",
    "            pred_idx = tf.argmax(logit,axis=-1)\n",
    "            ans_idx = tf.argmax(answer,axis=-1)\n",
    "            pred_attn_idx = tf.cast(tf.argmax(attn_score,axis=-1),tf.int32)\n",
    "            \n",
    "            true_list = tf.cast(tf.equal(pred_idx,ans_idx),tf.float32)\n",
    "            correct_ratio.append(float(tf.reduce_sum(true_list) / true_list.shape[0]))\n",
    "            \n",
    "            true_attn_list = tf.cast(tf.equal(pred_attn_idx,support),tf.float32)\n",
    "            correct_attn_ratio.append(float(tf.reduce_sum(true_attn_list) / true_attn_list.shape[0]))\n",
    "\n",
    "        correct_ratio = sum(correct_ratio) / len(correct_ratio) \n",
    "        correct_attn_ratio = sum(correct_attn_ratio) / len(correct_attn_ratio) \n",
    "        print('correct ratio: {0:.4f} correct attn ratio: {1:.4f}'.format(correct_ratio, correct_attn_ratio))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제 정답률은 좋아지고,  attention score도 높아진다! 좀 더 높일 수 있을까? 복잡한 문제에선 잘 작동할까? TODO: multi-hop attention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[9 0 2 5 0 7 6 1 0 4 7 4 4 5 3 4 4 1 1 0 1 0 0 1 2 8 1 0 3 7 2 5 1 5 1 5 4\n",
      " 3 6 0 2 9 8 0 6 0 5 6 5 7 3 1 0 5 5 2 3 5 1 3 5 0 9 5 1 5 5 0 4 7 5 6 5 7\n",
      " 7 5 1 2 6 7 1 3 7 4 7 2 5 7 0 5 3 5 1 0 3 7 2 5 1 5 0 1 3 1 8 7 6 3 0 1 9\n",
      " 5 5 5 1 0 2 6 4 5 9 1 3 6 8 2 1 3 2 6 3 1 2 1 3 1 4 5 2 9 5 5 5 8 0 3 6 7\n",
      " 1 0 7 1 1 4 0 7 1 3 7 7 6 9 5 0 0 5 8 1 9 2 4 9 1 5 3 1 6 3 2 5 9 1 3 6 1\n",
      " 8 6 9 4 1 5 1 3 1 6 7 5 5 4 8 6 6 4 8 3 5 2 3 5 6 7 1 2 7 5 1 7 1 1 1 9 3\n",
      " 1 7 7 0 0 4 3 1 1 1 0 5 9 8 9 9 0 1 1 3 3 3 0 1 3 7 8 0], shape=(250,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[8 0 2 5 0 2 6 1 0 4 7 0 2 5 3 3 1 1 0 0 1 0 0 0 2 7 1 0 3 7 2 5 1 5 1 5 4\n",
      " 2 5 0 0 0 8 0 6 0 5 5 3 4 3 1 0 3 5 2 3 3 1 3 3 0 1 5 0 3 2 0 4 3 3 6 5 1\n",
      " 6 5 1 2 3 5 1 3 3 1 7 2 5 2 0 2 3 4 1 0 3 3 2 5 1 5 0 1 2 0 8 7 6 1 0 1 5\n",
      " 5 1 5 1 0 2 6 4 5 6 1 3 6 1 2 1 3 2 6 3 1 2 1 3 1 4 5 2 5 2 5 5 8 0 2 3 3\n",
      " 1 0 7 1 1 4 0 4 1 3 5 7 6 5 5 0 0 2 0 1 9 0 4 5 1 5 3 1 6 2 1 0 8 1 3 2 1\n",
      " 4 3 5 3 1 5 1 2 1 1 1 4 3 3 8 5 3 4 3 3 5 2 0 5 5 1 1 2 7 5 1 3 1 1 1 9 3\n",
      " 1 3 6 0 0 0 3 1 1 1 0 5 6 6 9 1 0 1 1 2 1 0 0 1 2 4 8 0], shape=(250,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[4 4 2 2 4 6 0 6 3 1 1 1 5 2 5 7 3 3 3 7 1 1 5 6 1 7 9 3 7 6 1 5 4 3 9 5 2\n",
      " 5 2 0 4 1 5 1 6 7 9 9 0 1 2 5 1 8 0 9 0 2 0 7 4 2 1 0 1 8 9 7 3 2 9 1 4 1\n",
      " 5 3 1 6 6 0 3 5 9 8 0 6 1 9 0 6 3 9 3 8 3 9 3 5 6 0 0 1 1 9 5 3 1 5 5 5 3\n",
      " 5 1 5 9 1 7 3 4 4 6 1 1 9 0 1 6 7 6 5 1 2 2 9 0 6 1 1 9 5 5 6 9 9 1 5 4 6\n",
      " 1 2 1 0 2 3 2 5 3 3 4 1 1 7 2 5 4 5 2 7 5 0 0 3 1 6 5 3 1 7 0 7 5 8 3 4 7\n",
      " 5 6 1 5 2 7 5 1 5 1 5 6 9 3 7 7 0 5 1 1 7 1 7 6 1 5 9 6 5 3 5 7 9 0 6 1 5\n",
      " 2 5 6 8 3 1 2 4 3 8 0 2 3 9 7 0 0 1 9 9 1 9 9 1 0 6 6 2], shape=(250,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[4 4 2 0 3 2 0 5 2 1 1 0 5 2 4 7 2 2 3 7 1 1 2 6 1 3 4 2 2 6 1 5 0 3 9 4 1\n",
      " 5 2 0 2 1 5 0 6 1 4 9 0 1 2 1 1 4 0 9 0 1 0 7 4 0 1 0 1 8 1 2 3 0 7 0 4 1\n",
      " 2 2 1 1 3 0 3 3 9 7 0 6 0 1 0 2 2 7 2 2 3 0 3 5 6 0 0 1 1 3 0 1 1 5 3 5 3\n",
      " 4 0 5 6 1 7 2 4 4 1 1 0 7 0 1 0 7 2 1 1 2 0 9 0 6 1 1 4 3 5 4 5 9 1 5 4 6\n",
      " 1 2 0 0 2 3 1 4 3 3 3 1 1 5 0 5 4 2 2 3 5 0 0 3 1 4 5 3 1 2 0 1 3 8 3 4 6\n",
      " 3 4 1 5 2 2 5 1 5 0 4 2 4 0 7 4 0 5 1 1 3 0 4 3 1 4 0 3 5 3 5 3 5 0 6 1 0\n",
      " 0 0 2 6 3 0 0 4 3 0 0 2 2 2 6 0 0 1 4 9 1 1 9 0 0 2 6 2], shape=(250,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[4 8 1 7 0 4 6 7 3 4 6 1 5 0 4 1 3 2 8 3 9 2 5 6 1 4 1 4 7 7 3 6 0 3 0 6 7\n",
      " 5 5 5 4 9 1 7 9 3 0 1 4 6 0 1 5 2 7 8 6 5 5 3 2 2 2 7 1 1 5 2 1 7 3 1 3 6\n",
      " 6 7 1 4 4 1 1 9 5 1 6 2 1 3 8 5 5 3 3 0 0 5 5 5 7 7 4 9 3 9 6 1 1 0 0 1 7\n",
      " 3 9 4 9 5 5 3 1 9 7 8 6 7 1 7 0 1 7 2 0 0 1 3 5 3 0 0 0 5 4 0 1 9 2 2 7 7\n",
      " 0 4 1 2 1 8 1 9 0 7 5 1 0 4 6 1 4 7 4 1 1 4 1 9 7 1 3 8 3 5 3 5 7 4 0 5 7\n",
      " 0 7 5 6 1 3 8 4 3 5 3 4 3 7 1 3 3 5 3 4 1 1 6 1 4 1 8 3 0 3 1 6 4 3 3 7 3\n",
      " 9 4 9 7 3 9 3 1 9 1 0 0 3 7 5 0 0 5 3 1 9 2 4 3 5 7 2 0], shape=(250,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[3 7 1 6 0 2 6 7 2 3 6 1 3 0 0 0 2 0 4 2 0 2 5 3 1 4 1 4 5 1 2 5 0 3 0 5 5\n",
      " 5 0 3 1 8 1 7 4 2 0 1 4 5 0 0 5 2 0 8 0 3 5 3 0 1 0 4 1 1 4 1 1 1 3 1 0 2\n",
      " 6 3 1 4 4 1 1 7 5 1 5 2 1 3 3 2 3 2 3 0 0 0 5 5 3 7 4 9 1 2 6 1 0 0 0 1 2\n",
      " 3 9 4 5 1 5 1 1 1 7 0 6 3 1 0 0 0 2 0 0 0 1 3 4 3 0 0 0 5 4 0 1 6 2 2 7 6\n",
      " 0 4 1 2 1 8 1 9 0 7 2 0 0 4 1 1 2 6 0 1 1 4 1 3 7 1 3 5 3 0 1 5 5 3 0 2 2\n",
      " 0 1 5 3 1 3 5 3 2 3 3 2 2 7 1 0 1 2 3 4 1 1 6 0 1 0 8 1 0 3 1 1 4 3 1 4 1\n",
      " 5 4 0 7 1 9 3 1 9 0 0 0 3 6 2 0 0 5 0 1 9 2 4 0 5 6 2 0], shape=(250,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[5 0 7 7 8 7 2 5 9 5 5 4 3 3 3 2 7 7 2 1 3 9 8 1 6 7 6 6 2 1 3 1 0 6 9 7 6\n",
      " 6 7 2 7 4 7 3 7 3 2 2 5 5 5 4 0 8 1 7 3 8 6 7 3 0 1 5 9 8 5 5 6 5 8 4 2 6\n",
      " 0 4 6 7 6 4 7 8 4 7 5 9 3 5 0 6 5 3 5 6 7 1 9 8 7 7 4 5 6 1 3 8 1 3 6 7 7\n",
      " 3 7 6 0 9 2 2 3 2 1 9 3 5 3 0 6 1 3 8 1 0 5 3 5 1 1 1 3 7 1 2 1 2 9 1 2 1\n",
      " 1 1 1 1 2 7 8 3 9 6 2 3 2 3 5 2 5 4 1 5 5 4 7 7 3 8 1 0 9 2 1 0 4 3 3 2 1\n",
      " 7 0 1 7 1 9 1 4 0 8 5 5 6 9 0 3 6 1 6 7 3 1 1 4 3 0 3 1 5 3 4 9 3 3 9 1 4\n",
      " 7 9 1 2 1 4 7 7 0 1 8 9 3 9 1 0 8 1 3 5 1 8 5 5 1 1 7 1], shape=(250,), dtype=int32)\n",
      "tf.Tensor(\n",
      "[5 0 7 4 8 4 2 5 5 0 5 4 2 3 3 2 7 5 1 0 3 1 8 1 4 7 6 3 2 1 3 1 0 3 5 5 3\n",
      " 4 7 2 7 4 7 1 7 3 2 2 5 5 5 3 0 8 0 7 2 1 3 4 3 0 1 4 9 8 3 4 3 5 3 4 2 6\n",
      " 0 4 6 7 6 0 7 8 4 1 5 9 2 5 0 5 5 3 4 0 7 1 5 8 5 2 4 0 3 1 3 6 1 3 2 1 7\n",
      " 3 6 5 0 9 2 0 3 2 1 5 3 5 3 0 6 1 3 6 1 0 1 0 5 1 1 1 2 1 1 2 1 2 1 1 2 0\n",
      " 1 1 1 1 1 1 5 3 5 6 2 3 2 2 5 1 5 4 1 4 4 1 7 3 3 8 1 0 9 2 1 0 0 3 3 2 0\n",
      " 2 0 1 3 1 7 1 4 0 6 4 5 4 9 0 3 3 1 5 2 3 1 1 0 3 0 3 0 5 0 2 5 3 3 9 1 2\n",
      " 7 9 1 0 1 4 1 7 0 1 8 1 3 5 1 0 4 0 2 5 1 2 2 0 0 1 6 1], shape=(250,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for batch in test_loader:\n",
    "    story,question,answer,support = batch[0], batch[1], batch[2], batch[3] \n",
    "    logit, attn_score = model(story, question)\n",
    "    pred_attn_idx = tf.cast(tf.argmax(attn_score,axis=-1),tf.int32)\n",
    "    print(support)\n",
    "    print(pred_attn_idx)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2] *",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
